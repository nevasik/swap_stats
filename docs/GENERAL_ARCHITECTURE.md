                ┌────────────────────────────────────────────────────────────────┐
                │                   ИСТОЧНИКИ СОБЫТИЙ (внешние)                 │
                │  • Индексер on-chain (EVM logs)  • Биржевые источники/адаптеры│
                └───────────────┬────────────────────────────────────────────────┘
                                │  JSON-события Swap (at-least-once доставка)
                                ▼
                    ┌───────────────────────┐   Kafka API
                    │   Redpanda (Kafka)    │◄━━━━━━━━━━━━━━━━┐
                    │   topic: raw-swaps    │                 │
                    └──────────┬────────────┘                 │
                               │ Consumer Group               │
                               ▼                              │(авто-создание топика/retention)
                  ┌──────────────────────────────┐
                  │     Aggregator (наш svc)     │
                  │  ───────────────────────────  │
                  │  1) Dedup (Redis SETNX)       │
                  │  2) Window Engine (5m/1h/24h) │
                  │  3) Snapshot+Offsets (Redis)  │
                  │  4) ClickHouse batch writer   │
                  │  5) WS Hub + NATS broadcast   │
                  └──────┬───────────┬────────────┘
                         │           │
          «горячий state»│           │«срезы/патчи в реал-тайме»
                     ┌───▼───┐     ┌─▼─────────────────────────┐
                     │ Redis │     │           NATS            │
                     │  AOF  │     │ ws.broadcast.token.*      │
                     └───┬───┘     └───────────┬───────────────┘
                         │                    fan-out
                 snapshots+offsets               │
                         │                       │
                         │                ┌──────▼──────────┐
                         │                │   Другие инсты  │
                         │                │ Aggregator’ы    │
                         │                └───┬─────────────┘
                         │                    │ локальные WS-хабы
                         │                    ▼
                         │          ┌─────────────────────┐
                         │          │ WebSocket клиенты   │  (браузер/моб)
                         │          └─────────────────────┘
                         │
                         │ batched raw → аналитика/архив
                         ▼
               ┌──────────────────────────────┐
               │          ClickHouse          │
               │  raw_swaps + MV daily_agg    │  (TTL → S3)
               └───────┬──────────────────────┘
                       │  холодное хранение (через диск=’s3’)
                       ▼
               ┌──────────────────────────────┐
               │       S3 (MinIO, бакет)      │
               │        ч/з storage.xml       │
               └──────────────────────────────┘

         ┌──────────────────────────────────────────────────────────────────┐
         │         HTTP API (chi) + JWT(RS256) + RateLimit (Redis)          │
         │  /healthz /api/overview /api/tokens/:id/stats → из Window Engine │
         └──────────────────────────────────────────────────────────────────┘

         Метрики: Prometheus endpoint, pprof; Логи: stdout/driver

## Поток данных — шаг за шагом

1. Источник (индексер/адаптер) формирует JSON события swap и пишет в Redpanda (Kafka API) в топик raw-swaps.
Retention в топике — «горячий журнал» (дни/недели).

2. Aggregator (наш сервис) читает raw-swaps как consumer group:
   - парсит JSON → SwapEvent и строит канонический id event_id = chain:tx:logIndex; 
   - дедуп в Redis по SETNX с TTL (обычно 24h) — чтобы не учитывать дубликаты; 
   - применяет событие в окна (кольцевые минутные бакеты + rolling 5m/1h/24h) с учётом grace; 
   - батчем пишет «сырьё» в ClickHouse (raw_swaps); 
   - формирует патчи (срезы по токенам) и:
     - раздаёт их своим WebSocket-клиентам через локальный WS-hub (с коалесингом ~100 мс),
     - публикует те же патчи в NATS (ws.broadcast.token.*) — чтобы другие инстансы мгновенно раздали их своим клиентам.

3. Каждые несколько секунд Aggregator делает Snapshot горячего состояния (окна) + Offsets консьюмера в Redis.
При рестарте: Restore() → стартуем "тёплыми" (без перемалывания суток истории).

4. ClickHouse хранит «сырьё» и поддерживает материализованную витрину суточных агрегатов. По DDL настроен TTL:
   - горячие партиции остаются на локальном диске, 
   - старые — перемещаются на S3 (MinIO) в бакет (это наше «холодное хранилище»), 
   - ещё старше — удаляются (задаем в TTL).

5. HTTP API (chi) читает in-memory окна из Aggregator и отдаёт /overview и /tokens/:id/stats.
Доступ защищён JWT RS256 (публичный ключ у сервиса), поверх — rate-limit (Redis token-bucket по sub и IP).

6. Мониторинг: Prometheus-метрики (ingest/apply/lag/ws/ClickHouse batch и т.п.), pprof для профилирования.

## Что хранится и где
- Redpanda: только поток событий (временный журнал, retention); для пересчётов/реплея. 
- Redis:
  - dedupe:* — маркеры увиденных event_id (TTL ~24h),
  - snap:* — снапшот окон и оффсеты консьюмера (для тёплого старта), 
  - rl:* — токены rate-limit (JWT/IP).
- NATS: ничего долговременного — только эфемерные сообщения-патчи для fan-out между инстансами.
- ClickHouse:
  - raw_swaps — «сырьё» (90 дней, например),
  - daily_token_agg — суточная витрина (материализованная),
  - старшИе партиции → S3 (MinIO) согласно storage.xml + TTL.
- S3 (MinIO): «холодные» части партиций ClickHouse — дешёвое долговременное хранение.

## Два ключевых «пути»
 - Real-time путь: Redpanda → Aggregator → окна → WS-клиенты (+ NATS для мульти-инст).
 - Хранилище/аналитика: Redpanda → Aggregator → ClickHouse → (TTL) → S3.

## Безопасность и управление нагрузкой
- JWT RS256 на HTTP/WS: проверяем подпись публичным ключом; приватник лежит на стороне твоего Auth-issuer’а. 
- Rate-limit: Redis token-bucket (по sub и по IP) — защита API/WS от «шумных» клиентов. 
- Grace (на уровне окон): принимает слегка «запаздавшие» события без поломки real-time.

## Восстановление и «ровно-один-раз»
- At-least-once из Redpanda + дедуп в Redis ⇒ по факту учитываем один раз. 
- Snapshot + offsets ⇒ после рестарта состояние и позиция чтения согласованы.

### Метчинг по архитектуре:
1. ingest/ — Kafka consumer (franz-go/sarama).
2. window/ — кольцевые буферы и rolling 5m/1h/24h.
3. stores/clickhouse — коннект + батчер записи.
4. stores/redis — offsets/snapshot + dedup.
5. pubsub/nats — fan-out между инстансами.
6. api/http & api/ws — публичные API и WebSocket.
7. security/ — JWT (RS256 verify).
8. infra/ — docker-компоуз, ClickHouse storage.xml, MinIO, NATS конфиг.

# Почему выбрали ClickHouse вместо postgreSQL
Коротко: мы выбрали **ClickHouse**, потому что это колоночная БД, заточенная под **высокий ingest** и **быстрые сканы/агрегации по времени** на очень больших объёмах.
В твоём кейсе это 1000 EPS (≈86.4 млн событий/сутки) с постоянными запросами “сумма/кол-во за 5m/1h/24h”.
Для такого профиля CH даёт кратный выигрыш по **скорости** и **стоимости хранения** по сравнению с “голым”
PostgreSQL. Ниже — развернуто, с тем где PG тоже ок.

## Сравнение под нашу задачу

## 1) Профиль нагрузки
* **Ingest:** 1–3k событий/сек (батчами).
* **Запросы:** “сколько/где/за период”, группировки по токену/дню/часу, топы.
* **Холодное хранение:** 90+ дней, дешёвое S3.

### ClickHouse
* Колонки, векторное исполнение, агрегации “летят”.
* Пишем батчами 200–1000 строк → **десятки–сотни тыс. строк/сек** на обычном железе.
* Компрессия 3–5× → **\~7–10 ГБ/сутки** при 86.4M событий (оценка).
* Нативный **TTL + storage policy** → **автотиринг в S3**.

### PostgreSQL (row store)

* Обычные `INSERT` + индексы → write-amplification, **autovacuum** и bloat.
* Для time-series нужен **TimescaleDB** (расширение): hypertables, chunk’и, **continuous aggregates**. Это заметно помогает, но:

    * вставки всё равно дороже, чем у CH (особенно с индексами),
    * компрессия/стоимость хранения — обычно хуже,
    * **tiering в S3** нет “из коробки” (делается костылями: архив в Parquet, FDW, внешние пайплайны).

**Вывод:** при 86M+ строк/сутки CH ощутимо проще и дешевле в эксплуатации.

---

## 2) Типичные запросы
### В CH
```sql
-- объём за 24 часа по токену
SELECT sum(amount_usd), count()
FROM raw_swaps
WHERE token_address = '0x...' AND event_time >= now() - INTERVAL 24 HOUR;

-- топ токенов за 5 минут
SELECT token_address, sum(amount_usd) AS vol
FROM raw_swaps
WHERE event_time >= now() - INTERVAL 5 MINUTE
GROUP BY token_address
ORDER BY vol DESC
LIMIT 50;
```

Сканирует колонки, использует сортировку/первичный ключ для сужения диапазона — **быстро**.

### В PG/Timescale
Тоже можно, но для хорошей скорости придётся:
* тщательно проектировать **индексы по времени/токену**,
* поддерживать **continuous aggregates**,
* следить за **autovacuum** и **chunk management**.
  Это работает, но ручной работы больше, а потолок производительности ниже.

---
## 3) Операционка и стоимость
* **Хранение.** CH сжимает сильнее; 90 дней “горячо”, дальше **автоматически** в S3. В PG нативного tiering нет; делаешь DA/ELT в объектное хранилище сам.
* **Обслуживание.** У CH нет autovacuum, нет bloat в привычном PG-смысле; есть свои merge-процессы, но для append-only это просто и стабильно.
* **Стабильность latency.** Для “сканов” CH выигрывает. PG стабильно хорош, когда OLTP/индексные point-lookup’и.

---
## 4) Фичи под наш сценарий
* **Materialized Views** (CH) и двигатели типа **Summing/AggregatingMergeTree** — удобно держать готовые витрины (суточные, почасовые).
* **TTL + storage\_policy** — прозрачно уезжать в **S3** без внешних пайплайнов.
* **ReplacingMergeTree** — даёт “последнюю версию” события (если вдруг приходят апдейты/removals).
  (У нас raw почти append-only, но полезно для reorg и правок.)

PG/Timescale предлагает continuous aggregates и compression, но:
* не даёт нативного tiering,
* insert/aggregate throughput на таких объёмах будет ниже,
* в лицензировании Timescale есть нюансы по автorefresh/политикам (зависят от версии/edition).

---
## 5) Где **PostgreSQL** всё ещё лучше
* **OLTP/транзакции/сильная консистентность**: частые `UPDATE/DELETE`, строгие уникальные ключи, FK, сложные JOIN’ы — это дом PG.
* **Метаданные/конфиги/ACL**: идеально хранить в PG (или вообще в Redis), а не в CH.
* Если объёмы **маленькие** (например, <10M строк/день) и аналитика простая — TimescaleDB вполне можно тянуть “одной БД”.

Мы, кстати, и предлагаем гибрид: **CH для сырых событий и аналитики**, **PG/Redis** — для служебных данных (снапшоты, offsets, ACL, биллинг, квоты). Это даёт лучшее из обоих миров.

---
## 6) Риски/минусы ClickHouse
* Это не OLTP: **частые UPDATE/DELETE на отдельные строки** — не его сильная сторона. Делается через `ALTER DELETE/UPDATE` или движки типа ReplacingMergeTree/Collapsing, но это “массовые” операции, не транзакции уровня PG.
* **Вторичные индексы** в CH — не как в PG; проектирование “ключа сортировки” критично.
* Репликация/кластер требуют понимания MergeTree/ReplicatedMergeTree и бэкграунд-мержей (но для append-only — просто).

---
## 7) Итого
* **ClickHouse** — основной TS/аналитический стор: `raw_swaps` (90d → S3), суточные витрины. Максимальная скорость ingest, дешёвое хранение, минимум оперрей.
* **Redis** — дедуп, снапшоты, rate-limit.
* **PostgreSQL** (опционально) — “control-plane”: пользователи, ключи, тарифы/квоты, idempotency-токены, конфиги.
