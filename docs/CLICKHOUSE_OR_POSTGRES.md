Коротко: мы выбрали **ClickHouse**, потому что это колоночная БД, заточенная под **высокий ingest** и **быстрые сканы/агрегации по времени** на очень больших объёмах. 
В твоём кейсе это 1000 EPS (≈86.4 млн событий/сутки) с постоянными запросами “сумма/кол-во за 5m/1h/24h”. 
Для такого профиля CH даёт кратный выигрыш по **скорости** и **стоимости хранения** по сравнению с “голым” 
PostgreSQL. Ниже — развернуто, с тем где PG тоже ок.

# Сравнение под нашу задачу

## 1) Профиль нагрузки
* **Ingest:** 1–3k событий/сек (батчами).
* **Запросы:** “сколько/где/за период”, группировки по токену/дню/часу, топы.
* **Холодное хранение:** 90+ дней, дешёвое S3.

### ClickHouse
* Колонки, векторное исполнение, агрегации “летят”.
* Пишем батчами 200–1000 строк → **десятки–сотни тыс. строк/сек** на обычном железе.
* Компрессия 3–5× → **\~7–10 ГБ/сутки** при 86.4M событий (оценка).
* Нативный **TTL + storage policy** → **автотиринг в S3**.

### PostgreSQL (row store)

* Обычные `INSERT` + индексы → write-amplification, **autovacuum** и bloat.
* Для time-series нужен **TimescaleDB** (расширение): hypertables, chunk’и, **continuous aggregates**. Это заметно помогает, но:

    * вставки всё равно дороже, чем у CH (особенно с индексами),
    * компрессия/стоимость хранения — обычно хуже,
    * **tiering в S3** нет “из коробки” (делается костылями: архив в Parquet, FDW, внешние пайплайны).

**Вывод:** при 86M+ строк/сутки CH ощутимо проще и дешевле в эксплуатации.

---

## 2) Типичные запросы
### В CH
```sql
-- объём за 24 часа по токену
SELECT sum(amount_usd), count()
FROM raw_swaps
WHERE token_address = '0x...' AND event_time >= now() - INTERVAL 24 HOUR;

-- топ токенов за 5 минут
SELECT token_address, sum(amount_usd) AS vol
FROM raw_swaps
WHERE event_time >= now() - INTERVAL 5 MINUTE
GROUP BY token_address
ORDER BY vol DESC
LIMIT 50;
```

Сканирует колонки, использует сортировку/первичный ключ для сужения диапазона — **быстро**.

### В PG/Timescale
Тоже можно, но для хорошей скорости придётся:
* тщательно проектировать **индексы по времени/токену**,
* поддерживать **continuous aggregates**,
* следить за **autovacuum** и **chunk management**.
  Это работает, но ручной работы больше, а потолок производительности ниже.

---
## 3) Операционка и стоимость
* **Хранение.** CH сжимает сильнее; 90 дней “горячо”, дальше **автоматически** в S3. В PG нативного tiering нет; делаешь DA/ELT в объектное хранилище сам.
* **Обслуживание.** У CH нет autovacuum, нет bloat в привычном PG-смысле; есть свои merge-процессы, но для append-only это просто и стабильно.
* **Стабильность latency.** Для “сканов” CH выигрывает. PG стабильно хорош, когда OLTP/индексные point-lookup’и.

---
## 4) Фичи под наш сценарий
* **Materialized Views** (CH) и двигатели типа **Summing/AggregatingMergeTree** — удобно держать готовые витрины (суточные, почасовые).
* **TTL + storage\_policy** — прозрачно уезжать в **S3** без внешних пайплайнов.
* **ReplacingMergeTree** — даёт “последнюю версию” события (если вдруг приходят апдейты/removals).
  (У нас raw почти append-only, но полезно для reorg и правок.)

PG/Timescale предлагает continuous aggregates и compression, но:
* не даёт нативного tiering,
* insert/aggregate throughput на таких объёмах будет ниже,
* в лицензировании Timescale есть нюансы по автorefresh/политикам (зависят от версии/edition).

---
## 5) Где **PostgreSQL** всё ещё лучше
* **OLTP/транзакции/сильная консистентность**: частые `UPDATE/DELETE`, строгие уникальные ключи, FK, сложные JOIN’ы — это дом PG.
* **Метаданные/конфиги/ACL**: идеально хранить в PG (или вообще в Redis), а не в CH.
* Если объёмы **маленькие** (например, <10M строк/день) и аналитика простая — TimescaleDB вполне можно тянуть “одной БД”.

Мы, кстати, и предлагаем гибрид: **CH для сырых событий и аналитики**, **PG/Redis** — для служебных данных (снапшоты, offsets, ACL, биллинг, квоты). Это даёт лучшее из обоих миров.

---
## 6) Риски/минусы ClickHouse
* Это не OLTP: **частые UPDATE/DELETE на отдельные строки** — не его сильная сторона. Делается через `ALTER DELETE/UPDATE` или движки типа ReplacingMergeTree/Collapsing, но это “массовые” операции, не транзакции уровня PG.
* **Вторичные индексы** в CH — не как в PG; проектирование “ключа сортировки” критично.
* Репликация/кластер требуют понимания MergeTree/ReplicatedMergeTree и бэкграунд-мержей (но для append-only — просто).

---
## 7) Итого
* **ClickHouse** — основной TS/аналитический стор: `raw_swaps` (90d → S3), суточные витрины. Максимальная скорость ingest, дешёвое хранение, минимум оперрей.
* **Redis** — дедуп, снапшоты, rate-limit.
* **PostgreSQL** (опционально) — “control-plane”: пользователи, ключи, тарифы/квоты, idempotency-токены, конфиги.
